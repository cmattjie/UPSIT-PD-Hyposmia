{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21236\n",
      "366\n",
      "Using 2023 article cutoff\n",
      "Using 2023 article cutoff\n",
      "1504\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from database import filter_hyposmia\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "data_large = pd.read_csv('data/processed/remote_processed.csv', low_memory=False) #target = 0\n",
    "data_small = pd.read_csv('data/processed/clinical_processed_imputed.csv', low_memory=False) #target = 1\n",
    "\n",
    "print(len(data_large))\n",
    "print(len(data_small))\n",
    "\n",
    "# Filter hyposmia\n",
    "data_large = filter_hyposmia(data_large, '2023')\n",
    "data_small = filter_hyposmia(data_small, '2023')\n",
    "\n",
    "print(len(data_large))\n",
    "print(len(data_small))\n",
    "\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "\n",
    "# Ensure both datasets have age_var and sex_var columns\n",
    "assert age_var in data_large.columns and sex_var in data_large.columns\n",
    "assert age_var in data_small.columns and sex_var in data_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created with 388 instances.\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame to store the balanced dataset\n",
    "balanced_data = pd.DataFrame()\n",
    "\n",
    "# Set to keep track of used indices\n",
    "used_indices = set()\n",
    "\n",
    "# Loop through unique combinations of age and sex in the smaller dataset\n",
    "for (age, sex), group in data_small.groupby([age_var, sex_var]):\n",
    "    # Find all matching rows in the larger dataset by sex\n",
    "    matching_rows = data_large[data_large[sex_var] == sex]\n",
    "    \n",
    "    if len(matching_rows) > 0:\n",
    "        # Build a KDTree for the current sex group\n",
    "        kd_tree = KDTree(matching_rows[[age_var]].values)\n",
    "        \n",
    "        # Find the closest age values using KDTree\n",
    "        _, indices = kd_tree.query([[age]], k=len(matching_rows))\n",
    "        closest_indices = matching_rows.index[indices.flatten()]\n",
    "        \n",
    "        # Filter out already used indices\n",
    "        closest_indices = [idx for idx in closest_indices if idx not in used_indices]\n",
    "        \n",
    "        # If there are not enough unique closest indices, limit to available ones\n",
    "        closest_indices = closest_indices[:len(group)]\n",
    "        \n",
    "        # Update the set of used indices\n",
    "        used_indices.update(closest_indices)\n",
    "        \n",
    "        # Get the sampled rows\n",
    "        sampled_rows = data_large.loc[closest_indices]\n",
    "        \n",
    "        # Append the sampled rows to the balanced dataset\n",
    "        balanced_data = pd.concat([balanced_data, sampled_rows])\n",
    "    else:\n",
    "        # If no matching rows are found, you might want to log this or handle it differently\n",
    "        print(f\"No matching rows found for sex {sex}\")\n",
    "\n",
    "# Reset the index of the balanced dataset\n",
    "balanced_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Balanced dataset created with {len(balanced_data)*2} instances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 162)\n",
      "(194, 165)\n"
     ]
    }
   ],
   "source": [
    "print(balanced_data.shape)\n",
    "print(data_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.12886597938144 68.04278350515463\n",
      "5.635646332778791 5.687680941451203\n",
      "0.6701030927835051 0.6701030927835051\n",
      "0.47139193036274024 0.47139193036274024\n"
     ]
    }
   ],
   "source": [
    "#print mean and std of age and sex\n",
    "print(balanced_data['AGE'].mean(), data_small['AGE'].mean())\n",
    "print(balanced_data['AGE'].std(), data_small['AGE'].std())\n",
    "print(balanced_data['BIRTHSEX'].mean(), data_small['BIRTHSEX'].mean())\n",
    "print(balanced_data['BIRTHSEX'].std(), data_small['BIRTHSEX'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [PATNO, EVENT_ID, AGE, REC_ID_x, PAG_NAME_x, INFODT_x, AFICBERB, ASHKJEW, BASQUE, BIRTHDT, BIRTHSEX, CHLDBEAR, HOWLIVE, GAYLES, HETERO, BISEXUAL, PANSEXUAL, ASEXUAL, OTHSEXUALITY, HANDED, HISPLAT, RAASIAN, RABLACK, RAHAWOPI, RAINDALS, RANOS, RAWHITE, RAUNKNOWN, ORIG_ENTRY_x, LAST_UPDATE_x, EDUCYRS, REC_ID, PAG_NAME, INFODT, SCENT_01_CORRECT, SCENT_01_RESPONSE, SCENT_02_CORRECT, SCENT_02_RESPONSE, SCENT_03_CORRECT, SCENT_03_RESPONSE, SCENT_04_CORRECT, SCENT_04_RESPONSE, SCENT_05_CORRECT, SCENT_05_RESPONSE, SCENT_06_CORRECT, SCENT_06_RESPONSE, SCENT_07_CORRECT, SCENT_07_RESPONSE, SCENT_08_CORRECT, SCENT_08_RESPONSE, SCENT_09_CORRECT, SCENT_09_RESPONSE, SCENT_10_CORRECT, SCENT_10_RESPONSE, SCENT_11_CORRECT, SCENT_11_RESPONSE, SCENT_12_CORRECT, SCENT_12_RESPONSE, SCENT_13_CORRECT, SCENT_13_RESPONSE, SCENT_14_CORRECT, SCENT_14_RESPONSE, SCENT_15_CORRECT, SCENT_15_RESPONSE, SCENT_16_CORRECT, SCENT_16_RESPONSE, SCENT_17_CORRECT, SCENT_17_RESPONSE, SCENT_18_CORRECT, SCENT_18_RESPONSE, SCENT_19_CORRECT, SCENT_19_RESPONSE, SCENT_20_CORRECT, SCENT_20_RESPONSE, SCENT_21_CORRECT, SCENT_21_RESPONSE, SCENT_22_CORRECT, SCENT_22_RESPONSE, SCENT_23_CORRECT, SCENT_23_RESPONSE, SCENT_24_CORRECT, SCENT_24_RESPONSE, SCENT_25_CORRECT, SCENT_25_RESPONSE, SCENT_26_CORRECT, SCENT_26_RESPONSE, SCENT_27_CORRECT, SCENT_27_RESPONSE, SCENT_28_CORRECT, SCENT_28_RESPONSE, SCENT_29_CORRECT, SCENT_29_RESPONSE, SCENT_30_CORRECT, SCENT_30_RESPONSE, SCENT_31_CORRECT, SCENT_31_RESPONSE, SCENT_32_CORRECT, SCENT_32_RESPONSE, SCENT_33_CORRECT, SCENT_33_RESPONSE, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 165 columns]\n"
     ]
    }
   ],
   "source": [
    "#check for dupicate in PATNO in both datasets\n",
    "print(balanced_data['PATNO'].duplicated().sum())\n",
    "print(data_small['PATNO'].duplicated().sum())\n",
    "#print which patno is duplicate on data_small\n",
    "print(data_small[data_small['PATNO'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71230/3628960552.py:1: DtypeWarning: Columns (14,16,19,22,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  remote = pd.read_csv('data/processed/remote_processed.csv')\n",
      "100%|██████████| 194/194 [00:13<00:00, 14.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remote = pd.read_csv('data/processed/remote_processed.csv')\n",
    "from tqdm import tqdm\n",
    "\n",
    "#check, for each PATNO in data_balanced, if it is in remote, and if it has the equal values in all columns\n",
    "for i in tqdm(range(len(balanced_data))):\n",
    "    patno = balanced_data['PATNO'].iloc[i]\n",
    "    if patno in remote['PATNO'].values:\n",
    "        #check if values are equal for all columns\n",
    "        for col in balanced_data.columns:\n",
    "            if col != 'PATNO':\n",
    "                #ignore if both are nan\n",
    "                if pd.isnull(balanced_data[col].iloc[i]) and pd.isnull(remote[remote['PATNO'] == patno][col].values[0]):\n",
    "                    continue\n",
    "                if balanced_data[col].iloc[i] != remote[remote['PATNO'] == patno][col].values[0]:\n",
    "                    print(f\"Row {i} has different value in column {col}\")\n",
    "                    #print values\n",
    "                    print(balanced_data[col].iloc[i], remote[remote['PATNO'] == patno][col].values[0])\n",
    "    else:\n",
    "        print(f\"Row {i} has no matching PATNO {patno} in remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6709677419354839 0.6666666666666666\n",
      "0.470621394900324 0.47445571459117775\n",
      "310 78\n",
      "68.07322580645162 68.13589743589743\n",
      "5.789385380850459 5.118085436449702\n",
      "310 78\n",
      "0.4967741935483871 0.5128205128205128\n",
      "0.5007979852052057 0.5030708231255845\n",
      "310 78\n",
      "0.4967741935483871 0.5128205128205128\n",
      "0.5 0.5\n",
      "Balanced target\n",
      "0.6709677419354839 0.6666666666666666\n",
      "0.470621394900324 0.47445571459117764\n",
      "310 78\n",
      "68.03258064516129 68.2974358974359\n",
      "5.736738170751184 5.34568582843745\n",
      "310 78\n",
      "0.5 0.5\n",
      "0.5008084079652348 0.5032362797401965\n",
      "310 78\n",
      "(310, 222)\n",
      "(78, 222)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data_large = balanced_data\n",
    "data_small = pd.read_csv('data/processed/clinical_processed_imputed.csv')\n",
    "\n",
    "#filter for HYPOSMIA =1\n",
    "data_large = data_large[data_large['HYPOSMIA'] == 1]\n",
    "data_small = data_small[data_small['HYPOSMIA'] == 1]\n",
    "\n",
    "# Add a target column to each dataset\n",
    "data_large['target'] = 0\n",
    "data_small['target'] = 1\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([data_large, data_small])\n",
    "\n",
    "# Reset index of combined data\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the columns for stratification\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "target_var = 'target'\n",
    "\n",
    "# Ensure both datasets have age_var, sex_var, and target_var columns\n",
    "assert age_var in combined_data.columns and sex_var in combined_data.columns and target_var in combined_data.columns\n",
    "\n",
    "# Calculate percentiles for the age variable\n",
    "percentiles = np.percentile(combined_data[age_var], [20, 40, 60, 80])\n",
    "labels = ['0-20th', '21-40th', '41-60th', '61-80th', '81-100th']\n",
    "combined_data['age_binned'] = pd.cut(combined_data[age_var], bins=[combined_data[age_var].min()-1, *percentiles, combined_data[age_var].max()+1], labels=labels, include_lowest=True)\n",
    "\n",
    "# Create a stratification key\n",
    "combined_data['stratify_key'] = combined_data.apply(lambda row: f\"{row['age_binned']}_{row[sex_var]}_{row[target_var]}\", axis=1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = train_test_split(combined_data, test_size=0.2, stratify=combined_data['stratify_key'], random_state=42)\n",
    "\n",
    "# Drop the stratification key and binned age column\n",
    "train = train.drop(columns=['stratify_key', 'age_binned'])\n",
    "test = test.drop(columns=['stratify_key', 'age_binned'])\n",
    "\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "#switch random patients from train to test until target is balanced\n",
    "while abs(train['target'].mean() - test['target'].mean()) > 0.01:\n",
    "    #get random patient from train\n",
    "    random_patient = train.sample()\n",
    "    #switch to test\n",
    "    test = pd.concat([test, random_patient])\n",
    "    train = train.drop(random_patient.index)\n",
    "    #now switch from test to train\n",
    "    random_patient = test.sample()\n",
    "    train = pd.concat([train, random_patient])\n",
    "    test = test.drop(random_patient.index)\n",
    "    #print mean of target in train and test\n",
    "    print(train['target'].mean(), test['target'].mean())\n",
    "print (\"Balanced target\")\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "\n",
    "#print shapes\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(310, 222)\n",
      "(78, 222)\n",
      "Columns are the same\n",
      "PATNO\n",
      "EVENT_ID\n",
      "PAG_NAME\n",
      "INFODT\n",
      "HIQ1\n",
      "HIQ2\n",
      "HIQ3\n",
      "HIQ4\n",
      "HIQ5\n",
      "HIQ6\n",
      "ORIG_ENTRY\n",
      "LAST_UPDATE\n",
      "UPSITSNTDT\n",
      "UPSITENTDT\n",
      "SCRINTDT\n",
      "SCRINTST\n",
      "SCRTRDT\n",
      "SCRSITE\n",
      "SCRVISTYPE\n",
      "SCRSCHEDDT\n",
      "SCRVISST\n",
      "SCRRSNUM\n",
      "BLINTDT\n",
      "BLINTST\n",
      "BLTRDT\n",
      "BLSITE\n",
      "BLSCHEDDT\n",
      "BLVISST\n",
      "BLRSNUM\n",
      "COHORT\n",
      "TRACK\n",
      "CONSENTDT\n",
      "CONSENTST\n",
      "ENROLLST\n",
      "BIRTHSEX\n",
      "RAASIAN\n",
      "RABLACK\n",
      "RADEC\n",
      "RAHAWOPI\n",
      "RAINDALS\n",
      "RAUNK\n",
      "RAWHITE\n",
      "HISPLAT\n",
      "COUNTRY\n",
      "SCREENINGID\n",
      "SCREENDT\n",
      "SCREENTM\n",
      "CAMPAIGN\n",
      "REQAGE\n",
      "REQLOC\n",
      "DIAGPD\n",
      "PDMED\n",
      "DIAGAGE2\n",
      "RTRNLTR\n",
      "PARTNERID\n",
      "USQ1\n",
      "USQ2\n",
      "USQ3\n",
      "USQ4\n",
      "USQ5\n",
      "USQ6\n",
      "USQ7\n",
      "USQ8\n",
      "USQ8A\n",
      "USQ8B\n",
      "USQ8C\n",
      "USQ9\n",
      "USQ10\n",
      "REC_ID\n",
      "SCENT_01_CORRECT\n",
      "SCENT_01_RESPONSE\n",
      "SCENT_02_CORRECT\n",
      "SCENT_02_RESPONSE\n",
      "SCENT_03_CORRECT\n",
      "SCENT_03_RESPONSE\n",
      "SCENT_04_CORRECT\n",
      "SCENT_04_RESPONSE\n",
      "SCENT_05_CORRECT\n",
      "SCENT_05_RESPONSE\n",
      "SCENT_06_CORRECT\n",
      "SCENT_06_RESPONSE\n",
      "SCENT_07_CORRECT\n",
      "SCENT_07_RESPONSE\n",
      "SCENT_08_CORRECT\n",
      "SCENT_08_RESPONSE\n",
      "SCENT_09_CORRECT\n",
      "SCENT_09_RESPONSE\n",
      "SCENT_10_CORRECT\n",
      "SCENT_10_RESPONSE\n",
      "SCENT_11_CORRECT\n",
      "SCENT_11_RESPONSE\n",
      "SCENT_12_CORRECT\n",
      "SCENT_12_RESPONSE\n",
      "SCENT_13_CORRECT\n",
      "SCENT_13_RESPONSE\n",
      "SCENT_14_CORRECT\n",
      "SCENT_14_RESPONSE\n",
      "SCENT_15_CORRECT\n",
      "SCENT_15_RESPONSE\n",
      "SCENT_16_CORRECT\n",
      "SCENT_16_RESPONSE\n",
      "SCENT_17_CORRECT\n",
      "SCENT_17_RESPONSE\n",
      "SCENT_18_CORRECT\n",
      "SCENT_18_RESPONSE\n",
      "SCENT_19_CORRECT\n",
      "SCENT_19_RESPONSE\n",
      "SCENT_20_CORRECT\n",
      "SCENT_20_RESPONSE\n",
      "SCENT_21_CORRECT\n",
      "SCENT_21_RESPONSE\n",
      "SCENT_22_CORRECT\n",
      "SCENT_22_RESPONSE\n",
      "SCENT_23_CORRECT\n",
      "SCENT_23_RESPONSE\n",
      "SCENT_24_CORRECT\n",
      "SCENT_24_RESPONSE\n",
      "SCENT_25_CORRECT\n",
      "SCENT_25_RESPONSE\n",
      "SCENT_26_CORRECT\n",
      "SCENT_26_RESPONSE\n",
      "SCENT_27_CORRECT\n",
      "SCENT_27_RESPONSE\n",
      "SCENT_28_CORRECT\n",
      "SCENT_28_RESPONSE\n",
      "SCENT_29_CORRECT\n",
      "SCENT_29_RESPONSE\n",
      "SCENT_30_CORRECT\n",
      "SCENT_30_RESPONSE\n",
      "SCENT_31_CORRECT\n",
      "SCENT_31_RESPONSE\n",
      "SCENT_32_CORRECT\n",
      "SCENT_32_RESPONSE\n",
      "SCENT_33_CORRECT\n",
      "SCENT_33_RESPONSE\n",
      "SCENT_34_CORRECT\n",
      "SCENT_34_RESPONSE\n",
      "SCENT_35_CORRECT\n",
      "SCENT_35_RESPONSE\n",
      "SCENT_36_CORRECT\n",
      "SCENT_36_RESPONSE\n",
      "SCENT_37_CORRECT\n",
      "SCENT_37_RESPONSE\n",
      "SCENT_38_CORRECT\n",
      "SCENT_38_RESPONSE\n",
      "SCENT_39_CORRECT\n",
      "SCENT_39_RESPONSE\n",
      "SCENT_40_CORRECT\n",
      "SCENT_40_RESPONSE\n",
      "TOTAL_CORRECT\n",
      "upsitorder\n",
      "UPSITFORM\n",
      "UPSIT_PRCNTGE\n",
      "UPSIT_PRCTVER\n",
      "UPSIT_SOURCE\n",
      "UPSITLANGCNTR\n",
      "UPSIT_ORIG_ENTRY\n",
      "BIRTHDT\n",
      "YEAR\n",
      "AGE\n",
      "HYPOSMIA\n",
      "HYPOSMIA_PPMI\n",
      "target\n",
      "REC_ID_x\n",
      "PAG_NAME_x\n",
      "INFODT_x\n",
      "AFICBERB\n",
      "ASHKJEW\n",
      "BASQUE\n",
      "CHLDBEAR\n",
      "HOWLIVE\n",
      "GAYLES\n",
      "HETERO\n",
      "BISEXUAL\n",
      "PANSEXUAL\n",
      "ASEXUAL\n",
      "OTHSEXUALITY\n",
      "HANDED\n",
      "RANOS\n",
      "RAUNKNOWN\n",
      "ORIG_ENTRY_x\n",
      "LAST_UPDATE_x\n",
      "EDUCYRS\n",
      "COHORT_DEFINITION\n",
      "ENROLL_DATE\n",
      "ENROLL_STATUS\n",
      "STATUS_DATE\n",
      "ENROLL_AGE\n",
      "INEXPAGE\n",
      "AV133STDY\n",
      "TAUSTDY\n",
      "GAITSTDY\n",
      "PISTDY\n",
      "SV2ASTDY\n",
      "PPMI_ONLINE_ENROLL\n",
      "CONCOHORT\n",
      "CONCOHORT_DEFINITION\n",
      "CONLRRK2\n",
      "CONGBA\n",
      "CONSNCA\n",
      "CONPRKN\n",
      "CONPINK1\n",
      "CONHPSM\n",
      "CONRBD\n",
      "PHENOCNV\n",
      "DIAG1\n",
      "DIAG1VIS\n",
      "DIAG2\n",
      "DIAG2VIS\n",
      "DIAG3\n",
      "DIAG3VIS\n",
      "COMMENTS\n",
      "CONDATE\n",
      "ENRLPINK1\n",
      "ENRLPRKN\n",
      "ENRLSRDC\n",
      "ENRLHPSM\n",
      "ENRLRBD\n",
      "ENRLLRRK2\n",
      "ENRLSNCA\n",
      "ENRLGBA\n",
      "PD_Duration\n"
     ]
    }
   ],
   "source": [
    "#check if there is any duplicate in PATNO\n",
    "print(train['PATNO'].duplicated().sum())\n",
    "print(test['PATNO'].duplicated().sum())\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train_columns = train.columns\n",
    "test_columns = test.columns\n",
    "\n",
    "if train_columns.equals(test_columns):\n",
    "    print(\"Columns are the same\")\n",
    "\n",
    "for column in train_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.233064516129033 17.814102564102566\n",
      "4.0 7.0\n",
      "30.0 30.0\n",
      "target\n",
      "0    18.503226\n",
      "1    17.962903\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "target\n",
      "0    18.615385\n",
      "1    17.012821\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    4.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    7.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    30.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    30.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#sum all correct columns (that contain \"CORRECT\")\n",
    "\n",
    "#print mean and std of total_correct\n",
    "print(train['TOTAL_CORRECT'].mean(), test['TOTAL_CORRECT'].mean())\n",
    "#MIN AND MAX\n",
    "print(train['TOTAL_CORRECT'].min(), test['TOTAL_CORRECT'].min())\n",
    "print(train['TOTAL_CORRECT'].max(), test['TOTAL_CORRECT'].max())\n",
    "#print mean and std of TOTAL_CORRECT per target\n",
    "print(train.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "print(test.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "#print mean and std of TOTAL_CORRECT\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "#now per HYPOSMIA\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set created with 310 instances.\n",
      "Test set created with 78 instances.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the train and test sets to new CSV files\n",
    "train.to_csv('data/processed/train_data_hyposmia2023.csv', index=False)\n",
    "test.to_csv('data/processed/test_data_hyposmia2023.csv', index=False)\n",
    "\n",
    "print(f\"Train set created with {len(train)} instances.\")\n",
    "print(f\"Test set created with {len(test)} instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clinical data without imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21236\n",
      "248\n",
      "Using 2023 article cutoff\n",
      "Using 2023 article cutoff\n",
      "1504\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data_large = pd.read_csv('data/processed/remote_processed.csv', low_memory=False) #target = 0\n",
    "data_small = pd.read_csv('data/processed/clinical_processed.csv', low_memory=False) #target = 1\n",
    "\n",
    "print(len(data_large))\n",
    "print(len(data_small))\n",
    "\n",
    "# Filter hyposmia\n",
    "data_large = filter_hyposmia(data_large, '2023')\n",
    "data_small = filter_hyposmia(data_small, '2023')\n",
    "\n",
    "print(len(data_large))\n",
    "print(len(data_small))\n",
    "\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "\n",
    "# Ensure both datasets have age_var and sex_var columns\n",
    "assert age_var in data_large.columns and sex_var in data_large.columns\n",
    "assert age_var in data_small.columns and sex_var in data_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created with 250 instances.\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame to store the balanced dataset\n",
    "balanced_data = pd.DataFrame()\n",
    "\n",
    "# Set to keep track of used indices\n",
    "used_indices = set()\n",
    "\n",
    "# Loop through unique combinations of age and sex in the smaller dataset\n",
    "for (age, sex), group in data_small.groupby([age_var, sex_var]):\n",
    "    # Find all matching rows in the larger dataset by sex\n",
    "    matching_rows = data_large[data_large[sex_var] == sex]\n",
    "    \n",
    "    if len(matching_rows) > 0:\n",
    "        # Build a KDTree for the current sex group\n",
    "        kd_tree = KDTree(matching_rows[[age_var]].values)\n",
    "        \n",
    "        # Find the closest age values using KDTree\n",
    "        _, indices = kd_tree.query([[age]], k=len(matching_rows))\n",
    "        closest_indices = matching_rows.index[indices.flatten()]\n",
    "        \n",
    "        # Filter out already used indices\n",
    "        closest_indices = [idx for idx in closest_indices if idx not in used_indices]\n",
    "        \n",
    "        # If there are not enough unique closest indices, limit to available ones\n",
    "        closest_indices = closest_indices[:len(group)]\n",
    "        \n",
    "        # Update the set of used indices\n",
    "        used_indices.update(closest_indices)\n",
    "        \n",
    "        # Get the sampled rows\n",
    "        sampled_rows = data_large.loc[closest_indices]\n",
    "        \n",
    "        # Append the sampled rows to the balanced dataset\n",
    "        balanced_data = pd.concat([balanced_data, sampled_rows])\n",
    "    else:\n",
    "        # If no matching rows are found, you might want to log this or handle it differently\n",
    "        print(f\"No matching rows found for sex {sex}\")\n",
    "\n",
    "# Reset the index of the balanced dataset\n",
    "balanced_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Balanced dataset created with {len(balanced_data)*2} instances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 162)\n",
      "(125, 165)\n"
     ]
    }
   ],
   "source": [
    "print(balanced_data.shape)\n",
    "print(data_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.472 68.4096\n",
      "5.660525451501545 5.680064515762634\n",
      "0.672 0.672\n",
      "0.4713741066150078 0.47137410661500784\n"
     ]
    }
   ],
   "source": [
    "#print mean and std of age and sex\n",
    "print(balanced_data['AGE'].mean(), data_small['AGE'].mean())\n",
    "print(balanced_data['AGE'].std(), data_small['AGE'].std())\n",
    "print(balanced_data['BIRTHSEX'].mean(), data_small['BIRTHSEX'].mean())\n",
    "print(balanced_data['BIRTHSEX'].std(), data_small['BIRTHSEX'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Empty DataFrame\n",
      "Columns: [PATNO, EVENT_ID, AGE, REC_ID_x, PAG_NAME_x, INFODT_x, AFICBERB, ASHKJEW, BASQUE, BIRTHDT, BIRTHSEX, CHLDBEAR, HOWLIVE, GAYLES, HETERO, BISEXUAL, PANSEXUAL, ASEXUAL, OTHSEXUALITY, HANDED, HISPLAT, RAASIAN, RABLACK, RAHAWOPI, RAINDALS, RANOS, RAWHITE, RAUNKNOWN, ORIG_ENTRY_x, LAST_UPDATE_x, EDUCYRS, REC_ID, PAG_NAME, INFODT, SCENT_01_CORRECT, SCENT_01_RESPONSE, SCENT_02_CORRECT, SCENT_02_RESPONSE, SCENT_03_CORRECT, SCENT_03_RESPONSE, SCENT_04_CORRECT, SCENT_04_RESPONSE, SCENT_05_CORRECT, SCENT_05_RESPONSE, SCENT_06_CORRECT, SCENT_06_RESPONSE, SCENT_07_CORRECT, SCENT_07_RESPONSE, SCENT_08_CORRECT, SCENT_08_RESPONSE, SCENT_09_CORRECT, SCENT_09_RESPONSE, SCENT_10_CORRECT, SCENT_10_RESPONSE, SCENT_11_CORRECT, SCENT_11_RESPONSE, SCENT_12_CORRECT, SCENT_12_RESPONSE, SCENT_13_CORRECT, SCENT_13_RESPONSE, SCENT_14_CORRECT, SCENT_14_RESPONSE, SCENT_15_CORRECT, SCENT_15_RESPONSE, SCENT_16_CORRECT, SCENT_16_RESPONSE, SCENT_17_CORRECT, SCENT_17_RESPONSE, SCENT_18_CORRECT, SCENT_18_RESPONSE, SCENT_19_CORRECT, SCENT_19_RESPONSE, SCENT_20_CORRECT, SCENT_20_RESPONSE, SCENT_21_CORRECT, SCENT_21_RESPONSE, SCENT_22_CORRECT, SCENT_22_RESPONSE, SCENT_23_CORRECT, SCENT_23_RESPONSE, SCENT_24_CORRECT, SCENT_24_RESPONSE, SCENT_25_CORRECT, SCENT_25_RESPONSE, SCENT_26_CORRECT, SCENT_26_RESPONSE, SCENT_27_CORRECT, SCENT_27_RESPONSE, SCENT_28_CORRECT, SCENT_28_RESPONSE, SCENT_29_CORRECT, SCENT_29_RESPONSE, SCENT_30_CORRECT, SCENT_30_RESPONSE, SCENT_31_CORRECT, SCENT_31_RESPONSE, SCENT_32_CORRECT, SCENT_32_RESPONSE, SCENT_33_CORRECT, SCENT_33_RESPONSE, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 165 columns]\n"
     ]
    }
   ],
   "source": [
    "#check for dupicate in PATNO in both datasets\n",
    "print(balanced_data['PATNO'].duplicated().sum())\n",
    "print(data_small['PATNO'].duplicated().sum())\n",
    "#print which patno is duplicate on data_small\n",
    "print(data_small[data_small['PATNO'].duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71230/3628960552.py:1: DtypeWarning: Columns (14,16,19,22,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  remote = pd.read_csv('data/processed/remote_processed.csv')\n",
      "100%|██████████| 125/125 [00:08<00:00, 14.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remote = pd.read_csv('data/processed/remote_processed.csv')\n",
    "from tqdm import tqdm\n",
    "\n",
    "#check, for each PATNO in data_balanced, if it is in remote, and if it has the equal values in all columns\n",
    "for i in tqdm(range(len(balanced_data))):\n",
    "    patno = balanced_data['PATNO'].iloc[i]\n",
    "    if patno in remote['PATNO'].values:\n",
    "        #check if values are equal for all columns\n",
    "        for col in balanced_data.columns:\n",
    "            if col != 'PATNO':\n",
    "                #ignore if both are nan\n",
    "                if pd.isnull(balanced_data[col].iloc[i]) and pd.isnull(remote[remote['PATNO'] == patno][col].values[0]):\n",
    "                    continue\n",
    "                if balanced_data[col].iloc[i] != remote[remote['PATNO'] == patno][col].values[0]:\n",
    "                    print(f\"Row {i} has different value in column {col}\")\n",
    "                    #print values\n",
    "                    print(balanced_data[col].iloc[i], remote[remote['PATNO'] == patno][col].values[0])\n",
    "    else:\n",
    "        print(f\"Row {i} has no matching PATNO {patno} in remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67 0.68\n",
      "0.4713926763075365 0.47121207149916117\n",
      "200 50\n",
      "68.5315 68.078\n",
      "5.753037890964597 5.30576467396644\n",
      "200 50\n",
      "0.5 0.5\n",
      "0.5012547071170855 0.5050762722761054\n",
      "200 50\n",
      "Balanced target\n",
      "0.67 0.68\n",
      "0.4713926763075365 0.47121207149916117\n",
      "200 50\n",
      "68.5315 68.078\n",
      "5.753037890964597 5.30576467396644\n",
      "200 50\n",
      "0.5 0.5\n",
      "0.5012547071170855 0.5050762722761054\n",
      "200 50\n",
      "(200, 222)\n",
      "(50, 222)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data_large = balanced_data\n",
    "data_small = pd.read_csv('data/processed/clinical_processed.csv')\n",
    "\n",
    "#filter for HYPOSMIA =1\n",
    "data_large = data_large[data_large['HYPOSMIA'] == 1]\n",
    "data_small = data_small[data_small['HYPOSMIA'] == 1]\n",
    "\n",
    "# Add a target column to each dataset\n",
    "data_large['target'] = 0\n",
    "data_small['target'] = 1\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([data_large, data_small])\n",
    "\n",
    "# Reset index of combined data\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the columns for stratification\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "target_var = 'target'\n",
    "\n",
    "# Ensure both datasets have age_var, sex_var, and target_var columns\n",
    "assert age_var in combined_data.columns and sex_var in combined_data.columns and target_var in combined_data.columns\n",
    "\n",
    "# Calculate percentiles for the age variable\n",
    "percentiles = np.percentile(combined_data[age_var], [20, 40, 60, 80])\n",
    "labels = ['0-20th', '21-40th', '41-60th', '61-80th', '81-100th']\n",
    "combined_data['age_binned'] = pd.cut(combined_data[age_var], bins=[combined_data[age_var].min()-1, *percentiles, combined_data[age_var].max()+1], labels=labels, include_lowest=True)\n",
    "\n",
    "# Create a stratification key\n",
    "combined_data['stratify_key'] = combined_data.apply(lambda row: f\"{row['age_binned']}_{row[sex_var]}_{row[target_var]}\", axis=1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = train_test_split(combined_data, test_size=0.2, stratify=combined_data['stratify_key'], random_state=42)\n",
    "\n",
    "# Drop the stratification key and binned age column\n",
    "train = train.drop(columns=['stratify_key', 'age_binned'])\n",
    "test = test.drop(columns=['stratify_key', 'age_binned'])\n",
    "\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "#switch random patients from train to test until target is balanced\n",
    "while abs(train['target'].mean() - test['target'].mean()) > 0.01:\n",
    "    #get random patient from train\n",
    "    random_patient = train.sample()\n",
    "    #switch to test\n",
    "    test = pd.concat([test, random_patient])\n",
    "    train = train.drop(random_patient.index)\n",
    "    #now switch from test to train\n",
    "    random_patient = test.sample()\n",
    "    train = pd.concat([train, random_patient])\n",
    "    test = test.drop(random_patient.index)\n",
    "    #print mean of target in train and test\n",
    "    print(train['target'].mean(), test['target'].mean())\n",
    "print (\"Balanced target\")\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "\n",
    "#print shapes\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(200, 222)\n",
      "(50, 222)\n",
      "Columns are the same\n",
      "PATNO\n",
      "EVENT_ID\n",
      "PAG_NAME\n",
      "INFODT\n",
      "HIQ1\n",
      "HIQ2\n",
      "HIQ3\n",
      "HIQ4\n",
      "HIQ5\n",
      "HIQ6\n",
      "ORIG_ENTRY\n",
      "LAST_UPDATE\n",
      "UPSITSNTDT\n",
      "UPSITENTDT\n",
      "SCRINTDT\n",
      "SCRINTST\n",
      "SCRTRDT\n",
      "SCRSITE\n",
      "SCRVISTYPE\n",
      "SCRSCHEDDT\n",
      "SCRVISST\n",
      "SCRRSNUM\n",
      "BLINTDT\n",
      "BLINTST\n",
      "BLTRDT\n",
      "BLSITE\n",
      "BLSCHEDDT\n",
      "BLVISST\n",
      "BLRSNUM\n",
      "COHORT\n",
      "TRACK\n",
      "CONSENTDT\n",
      "CONSENTST\n",
      "ENROLLST\n",
      "BIRTHSEX\n",
      "RAASIAN\n",
      "RABLACK\n",
      "RADEC\n",
      "RAHAWOPI\n",
      "RAINDALS\n",
      "RAUNK\n",
      "RAWHITE\n",
      "HISPLAT\n",
      "COUNTRY\n",
      "SCREENINGID\n",
      "SCREENDT\n",
      "SCREENTM\n",
      "CAMPAIGN\n",
      "REQAGE\n",
      "REQLOC\n",
      "DIAGPD\n",
      "PDMED\n",
      "DIAGAGE2\n",
      "RTRNLTR\n",
      "PARTNERID\n",
      "USQ1\n",
      "USQ2\n",
      "USQ3\n",
      "USQ4\n",
      "USQ5\n",
      "USQ6\n",
      "USQ7\n",
      "USQ8\n",
      "USQ8A\n",
      "USQ8B\n",
      "USQ8C\n",
      "USQ9\n",
      "USQ10\n",
      "REC_ID\n",
      "SCENT_01_CORRECT\n",
      "SCENT_01_RESPONSE\n",
      "SCENT_02_CORRECT\n",
      "SCENT_02_RESPONSE\n",
      "SCENT_03_CORRECT\n",
      "SCENT_03_RESPONSE\n",
      "SCENT_04_CORRECT\n",
      "SCENT_04_RESPONSE\n",
      "SCENT_05_CORRECT\n",
      "SCENT_05_RESPONSE\n",
      "SCENT_06_CORRECT\n",
      "SCENT_06_RESPONSE\n",
      "SCENT_07_CORRECT\n",
      "SCENT_07_RESPONSE\n",
      "SCENT_08_CORRECT\n",
      "SCENT_08_RESPONSE\n",
      "SCENT_09_CORRECT\n",
      "SCENT_09_RESPONSE\n",
      "SCENT_10_CORRECT\n",
      "SCENT_10_RESPONSE\n",
      "SCENT_11_CORRECT\n",
      "SCENT_11_RESPONSE\n",
      "SCENT_12_CORRECT\n",
      "SCENT_12_RESPONSE\n",
      "SCENT_13_CORRECT\n",
      "SCENT_13_RESPONSE\n",
      "SCENT_14_CORRECT\n",
      "SCENT_14_RESPONSE\n",
      "SCENT_15_CORRECT\n",
      "SCENT_15_RESPONSE\n",
      "SCENT_16_CORRECT\n",
      "SCENT_16_RESPONSE\n",
      "SCENT_17_CORRECT\n",
      "SCENT_17_RESPONSE\n",
      "SCENT_18_CORRECT\n",
      "SCENT_18_RESPONSE\n",
      "SCENT_19_CORRECT\n",
      "SCENT_19_RESPONSE\n",
      "SCENT_20_CORRECT\n",
      "SCENT_20_RESPONSE\n",
      "SCENT_21_CORRECT\n",
      "SCENT_21_RESPONSE\n",
      "SCENT_22_CORRECT\n",
      "SCENT_22_RESPONSE\n",
      "SCENT_23_CORRECT\n",
      "SCENT_23_RESPONSE\n",
      "SCENT_24_CORRECT\n",
      "SCENT_24_RESPONSE\n",
      "SCENT_25_CORRECT\n",
      "SCENT_25_RESPONSE\n",
      "SCENT_26_CORRECT\n",
      "SCENT_26_RESPONSE\n",
      "SCENT_27_CORRECT\n",
      "SCENT_27_RESPONSE\n",
      "SCENT_28_CORRECT\n",
      "SCENT_28_RESPONSE\n",
      "SCENT_29_CORRECT\n",
      "SCENT_29_RESPONSE\n",
      "SCENT_30_CORRECT\n",
      "SCENT_30_RESPONSE\n",
      "SCENT_31_CORRECT\n",
      "SCENT_31_RESPONSE\n",
      "SCENT_32_CORRECT\n",
      "SCENT_32_RESPONSE\n",
      "SCENT_33_CORRECT\n",
      "SCENT_33_RESPONSE\n",
      "SCENT_34_CORRECT\n",
      "SCENT_34_RESPONSE\n",
      "SCENT_35_CORRECT\n",
      "SCENT_35_RESPONSE\n",
      "SCENT_36_CORRECT\n",
      "SCENT_36_RESPONSE\n",
      "SCENT_37_CORRECT\n",
      "SCENT_37_RESPONSE\n",
      "SCENT_38_CORRECT\n",
      "SCENT_38_RESPONSE\n",
      "SCENT_39_CORRECT\n",
      "SCENT_39_RESPONSE\n",
      "SCENT_40_CORRECT\n",
      "SCENT_40_RESPONSE\n",
      "TOTAL_CORRECT\n",
      "upsitorder\n",
      "UPSITFORM\n",
      "UPSIT_PRCNTGE\n",
      "UPSIT_PRCTVER\n",
      "UPSIT_SOURCE\n",
      "UPSITLANGCNTR\n",
      "UPSIT_ORIG_ENTRY\n",
      "BIRTHDT\n",
      "YEAR\n",
      "AGE\n",
      "HYPOSMIA\n",
      "HYPOSMIA_PPMI\n",
      "target\n",
      "REC_ID_x\n",
      "PAG_NAME_x\n",
      "INFODT_x\n",
      "AFICBERB\n",
      "ASHKJEW\n",
      "BASQUE\n",
      "CHLDBEAR\n",
      "HOWLIVE\n",
      "GAYLES\n",
      "HETERO\n",
      "BISEXUAL\n",
      "PANSEXUAL\n",
      "ASEXUAL\n",
      "OTHSEXUALITY\n",
      "HANDED\n",
      "RANOS\n",
      "RAUNKNOWN\n",
      "ORIG_ENTRY_x\n",
      "LAST_UPDATE_x\n",
      "EDUCYRS\n",
      "COHORT_DEFINITION\n",
      "ENROLL_DATE\n",
      "ENROLL_STATUS\n",
      "STATUS_DATE\n",
      "ENROLL_AGE\n",
      "INEXPAGE\n",
      "AV133STDY\n",
      "TAUSTDY\n",
      "GAITSTDY\n",
      "PISTDY\n",
      "SV2ASTDY\n",
      "PPMI_ONLINE_ENROLL\n",
      "CONCOHORT\n",
      "CONCOHORT_DEFINITION\n",
      "CONLRRK2\n",
      "CONGBA\n",
      "CONSNCA\n",
      "CONPRKN\n",
      "CONPINK1\n",
      "CONHPSM\n",
      "CONRBD\n",
      "PHENOCNV\n",
      "DIAG1\n",
      "DIAG1VIS\n",
      "DIAG2\n",
      "DIAG2VIS\n",
      "DIAG3\n",
      "DIAG3VIS\n",
      "COMMENTS\n",
      "CONDATE\n",
      "ENRLPINK1\n",
      "ENRLPRKN\n",
      "ENRLSRDC\n",
      "ENRLHPSM\n",
      "ENRLRBD\n",
      "ENRLLRRK2\n",
      "ENRLSNCA\n",
      "ENRLGBA\n",
      "PD_Duration\n"
     ]
    }
   ],
   "source": [
    "#check if there is any duplicate in PATNO\n",
    "print(train['PATNO'].duplicated().sum())\n",
    "print(test['PATNO'].duplicated().sum())\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train_columns = train.columns\n",
    "test_columns = test.columns\n",
    "\n",
    "if train_columns.equals(test_columns):\n",
    "    print(\"Columns are the same\")\n",
    "\n",
    "for column in train_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.73 16.7\n",
      "6.0 7.0\n",
      "30.0 27.0\n",
      "target\n",
      "0    18.19\n",
      "1    17.27\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "target\n",
      "0    18.44\n",
      "1    14.96\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    6.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    7.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    30.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "1.0    27.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#sum all correct columns (that contain \"CORRECT\")\n",
    "\n",
    "#print mean and std of total_correct\n",
    "print(train['TOTAL_CORRECT'].mean(), test['TOTAL_CORRECT'].mean())\n",
    "#MIN AND MAX\n",
    "print(train['TOTAL_CORRECT'].min(), test['TOTAL_CORRECT'].min())\n",
    "print(train['TOTAL_CORRECT'].max(), test['TOTAL_CORRECT'].max())\n",
    "#print mean and std of TOTAL_CORRECT per target\n",
    "print(train.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "print(test.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "#print mean and std of TOTAL_CORRECT\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "#now per HYPOSMIA\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set created with 200 instances.\n",
      "Test set created with 50 instances.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the train and test sets to new CSV files\n",
    "train.to_csv('data/processed/train_data_without_imput.csv', index=False)\n",
    "test.to_csv('data/processed/test_data_without_imput.csv', index=False)\n",
    "\n",
    "print(f\"Train set created with {len(train)} instances.\")\n",
    "print(f\"Test set created with {len(test)} instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT FILTERING FOR HYPOSMIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21236\n",
      "366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from database import filter_hyposmia\n",
    "import pandas as pd\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "data_large = pd.read_csv('data/processed/remote_processed.csv', low_memory=False) #target = 0\n",
    "data_small = pd.read_csv('data/processed/clinical_processed_imputed.csv', low_memory=False) #target = 1\n",
    "\n",
    "print(len(data_large))\n",
    "print(len(data_small))\n",
    "\n",
    "# Filter hyposmia\n",
    "#data_large = filter_hyposmia(data_large, '2023')\n",
    "#data_small = filter_hyposmia(data_small, '2023')\n",
    "\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "\n",
    "# Ensure both datasets have age_var and sex_var columns\n",
    "assert age_var in data_large.columns and sex_var in data_large.columns\n",
    "assert age_var in data_small.columns and sex_var in data_small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created with 732 instances.\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame to store the balanced dataset\n",
    "balanced_data = pd.DataFrame()\n",
    "\n",
    "# Set to keep track of used indices\n",
    "used_indices = set()\n",
    "\n",
    "# Loop through unique combinations of age and sex in the smaller dataset\n",
    "for (age, sex), group in data_small.groupby([age_var, sex_var]):\n",
    "    # Find all matching rows in the larger dataset by sex\n",
    "    matching_rows = data_large[data_large[sex_var] == sex]\n",
    "    \n",
    "    if len(matching_rows) > 0:\n",
    "        # Build a KDTree for the current sex group\n",
    "        kd_tree = KDTree(matching_rows[[age_var]].values)\n",
    "        \n",
    "        # Find the closest age values using KDTree\n",
    "        _, indices = kd_tree.query([[age]], k=len(matching_rows))\n",
    "        closest_indices = matching_rows.index[indices.flatten()]\n",
    "        \n",
    "        # Filter out already used indices\n",
    "        closest_indices = [idx for idx in closest_indices if idx not in used_indices]\n",
    "        \n",
    "        # If there are not enough unique closest indices, limit to available ones\n",
    "        closest_indices = closest_indices[:len(group)]\n",
    "        \n",
    "        # Update the set of used indices\n",
    "        used_indices.update(closest_indices)\n",
    "        \n",
    "        # Get the sampled rows\n",
    "        sampled_rows = data_large.loc[closest_indices]\n",
    "        \n",
    "        # Append the sampled rows to the balanced dataset\n",
    "        balanced_data = pd.concat([balanced_data, sampled_rows])\n",
    "    else:\n",
    "        # If no matching rows are found, you might want to log this or handle it differently\n",
    "        print(f\"No matching rows found for sex {sex}\")\n",
    "\n",
    "# Reset the index of the balanced dataset\n",
    "balanced_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Balanced dataset created with {len(balanced_data)*2} instances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366, 162)\n",
      "(366, 165)\n"
     ]
    }
   ],
   "source": [
    "print(balanced_data.shape)\n",
    "print(data_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.74043715846994 68.7448087431694\n",
      "5.858220200535136 5.861241426456602\n",
      "0.6338797814207651 0.6338797814207651\n",
      "0.482402352133065 0.482402352133065\n"
     ]
    }
   ],
   "source": [
    "#print mean and std of age and sex\n",
    "print(balanced_data['AGE'].mean(), data_small['AGE'].mean())\n",
    "print(balanced_data['AGE'].std(), data_small['AGE'].std())\n",
    "print(balanced_data['BIRTHSEX'].mean(), data_small['BIRTHSEX'].mean())\n",
    "print(balanced_data['BIRTHSEX'].std(), data_small['BIRTHSEX'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "      PATNO EVENT_ID   AGE  REC_ID_x PAG_NAME_x INFODT_x  AFICBERB  ASHKJEW  \\\n",
      "85   130769     BLSC  75.4   IA87748     SCREEN  06/2022       0.0      0.0   \n",
      "318  262152     BLSC  64.6  IA443252     SCREEN  03/2024       0.0      0.0   \n",
      "\n",
      "     BASQUE  BIRTHDT  ...  ENRLPRKN  ENRLSRDC  ENRLHPSM  ENRLRBD  ENRLLRRK2  \\\n",
      "85      0.0  03/1947  ...       0.0       1.0       0.0      0.0        0.0   \n",
      "318     0.0  09/1959  ...       0.0       1.0       0.0      0.0        0.0   \n",
      "\n",
      "     ENRLSNCA  ENRLGBA  HYPOSMIA  HYPOSMIA_PPMI  PD_Duration  \n",
      "85        0.0      0.0       0.0            0.0       -121.0  \n",
      "318       0.0      0.0       0.0            0.0         30.0  \n",
      "\n",
      "[2 rows x 165 columns]\n"
     ]
    }
   ],
   "source": [
    "#check for dupicate in PATNO in both datasets\n",
    "print(balanced_data['PATNO'].duplicated().sum())\n",
    "print(data_small['PATNO'].duplicated().sum())\n",
    "#print which patno is duplicate on data_small\n",
    "print(data_small[data_small['PATNO'].duplicated()])\n",
    "\n",
    "#drop duplicates\n",
    "data_small = data_small.drop_duplicates(subset='PATNO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71230/3628960552.py:1: DtypeWarning: Columns (14,16,19,22,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  remote = pd.read_csv('data/processed/remote_processed.csv')\n",
      "100%|██████████| 366/366 [00:26<00:00, 13.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "remote = pd.read_csv('data/processed/remote_processed.csv')\n",
    "from tqdm import tqdm\n",
    "\n",
    "#check, for each PATNO in data_balanced, if it is in remote, and if it has the equal values in all columns\n",
    "for i in tqdm(range(len(balanced_data))):\n",
    "    patno = balanced_data['PATNO'].iloc[i]\n",
    "    if patno in remote['PATNO'].values:\n",
    "        #check if values are equal for all columns\n",
    "        for col in balanced_data.columns:\n",
    "            if col != 'PATNO':\n",
    "                #ignore if both are nan\n",
    "                if pd.isnull(balanced_data[col].iloc[i]) and pd.isnull(remote[remote['PATNO'] == patno][col].values[0]):\n",
    "                    continue\n",
    "                if balanced_data[col].iloc[i] != remote[remote['PATNO'] == patno][col].values[0]:\n",
    "                    print(f\"Row {i} has different value in column {col}\")\n",
    "                    #print values\n",
    "                    print(balanced_data[col].iloc[i], remote[remote['PATNO'] == patno][col].values[0])\n",
    "    else:\n",
    "        print(f\"Row {i} has no matching PATNO {patno} in remote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6335616438356164 0.636986301369863\n",
      "0.4822442385632843 0.4825240674547979\n",
      "584 146\n",
      "68.72602739726027 68.79178082191783\n",
      "5.818077403372382 6.0281238214557336\n",
      "584 146\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.5004168942693765 0.501674098669036\n",
      "584 146\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.488013698630137 0.541095890410959\n",
      "0.488013698630137 0.541095890410959\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.488013698630137 0.541095890410959\n",
      "0.488013698630137 0.541095890410959\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.476027397260274 0.589041095890411\n",
      "0.476027397260274 0.589041095890411\n",
      "0.4743150684931507 0.5958904109589042\n",
      "0.4743150684931507 0.5958904109589042\n",
      "0.4743150684931507 0.5958904109589042\n",
      "0.476027397260274 0.589041095890411\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.476027397260274 0.589041095890411\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4777397260273973 0.5821917808219178\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4794520547945205 0.5753424657534246\n",
      "0.4811643835616438 0.5684931506849316\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4828767123287671 0.5616438356164384\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4845890410958904 0.5547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.488013698630137 0.541095890410959\n",
      "0.488013698630137 0.541095890410959\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.488013698630137 0.541095890410959\n",
      "0.488013698630137 0.541095890410959\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.4863013698630137 0.547945205479452\n",
      "0.488013698630137 0.541095890410959\n",
      "0.4897260273972603 0.5342465753424658\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4914383561643836 0.5273972602739726\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4931506849315068 0.5205479452054794\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4948630136986301 0.5136986301369864\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4965753424657534 0.5068493150684932\n",
      "0.4982876712328767 0.5\n",
      "Balanced target\n",
      "0.6181506849315068 0.6986301369863014\n",
      "0.4862564012105408 0.4604325254950764\n",
      "584 146\n",
      "68.84400684931506 68.31986301369864\n",
      "5.9513809205105055 5.460069721204637\n",
      "584 146\n",
      "0.4982876712328767 0.5\n",
      "0.5004256981485526 0.5017211754859809\n",
      "584 146\n",
      "(584, 222)\n",
      "(146, 222)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "data_large = balanced_data\n",
    "data_small = pd.read_csv('data/processed/clinical_processed_imputed.csv')\n",
    "\n",
    "#drop duplicates\n",
    "data_small = data_small.drop_duplicates(subset='PATNO')\n",
    "\n",
    "#filter for HYPOSMIA =1\n",
    "#data_large = data_large[data_large['HYPOSMIA'] == 1]\n",
    "#data_small = data_small[data_small['HYPOSMIA'] == 1]\n",
    "\n",
    "# Add a target column to each dataset\n",
    "data_large['target'] = 0\n",
    "data_small['target'] = 1\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([data_large, data_small])\n",
    "\n",
    "# Reset index of combined data\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the columns for stratification\n",
    "age_var = 'AGE'\n",
    "sex_var = 'BIRTHSEX'\n",
    "target_var = 'target'\n",
    "\n",
    "# Ensure both datasets have age_var, sex_var, and target_var columns\n",
    "assert age_var in combined_data.columns and sex_var in combined_data.columns and target_var in combined_data.columns\n",
    "\n",
    "# Calculate percentiles for the age variable\n",
    "percentiles = np.percentile(combined_data[age_var], [20, 40, 60, 80])\n",
    "labels = ['0-20th', '21-40th', '41-60th', '61-80th', '81-100th']\n",
    "combined_data['age_binned'] = pd.cut(combined_data[age_var], bins=[combined_data[age_var].min()-1, *percentiles, combined_data[age_var].max()+1], labels=labels, include_lowest=True)\n",
    "\n",
    "# Create a stratification key\n",
    "combined_data['stratify_key'] = combined_data.apply(lambda row: f\"{row['age_binned']}_{row[sex_var]}_{row[target_var]}\", axis=1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = train_test_split(combined_data, test_size=0.2, stratify=combined_data['stratify_key'], random_state=42)\n",
    "\n",
    "# Drop the stratification key and binned age column\n",
    "train = train.drop(columns=['stratify_key', 'age_binned'])\n",
    "test = test.drop(columns=['stratify_key', 'age_binned'])\n",
    "\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "#switch random patients from train to test until target is balanced\n",
    "while abs(train['target'].mean() - test['target'].mean()) > 0.01:\n",
    "    #get random patient from train\n",
    "    random_patient = train.sample()\n",
    "    #switch to test\n",
    "    test = pd.concat([test, random_patient])\n",
    "    train = train.drop(random_patient.index)\n",
    "    #now switch from test to train\n",
    "    random_patient = test.sample()\n",
    "    train = pd.concat([train, random_patient])\n",
    "    test = test.drop(random_patient.index)\n",
    "    #print mean of target in train and test\n",
    "    print(train['target'].mean(), test['target'].mean())\n",
    "print (\"Balanced target\")\n",
    "#print mean, std and count of sex, age and target in train and test\n",
    "print(train['BIRTHSEX'].mean(), test['BIRTHSEX'].mean())\n",
    "print(train['BIRTHSEX'].std(), test['BIRTHSEX'].std())\n",
    "print(train['BIRTHSEX'].count(), test['BIRTHSEX'].count())\n",
    "print(train['AGE'].mean(), test['AGE'].mean())\n",
    "print(train['AGE'].std(), test['AGE'].std())\n",
    "print(train['AGE'].count(), test['AGE'].count())\n",
    "print(train['target'].mean(), test['target'].mean())\n",
    "print(train['target'].std(), test['target'].std())\n",
    "print(train['target'].count(), test['target'].count())\n",
    "\n",
    "\n",
    "#print shapes\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(584, 222)\n",
      "(146, 222)\n",
      "Columns are the same\n",
      "PATNO\n",
      "EVENT_ID\n",
      "PAG_NAME\n",
      "INFODT\n",
      "HIQ1\n",
      "HIQ2\n",
      "HIQ3\n",
      "HIQ4\n",
      "HIQ5\n",
      "HIQ6\n",
      "ORIG_ENTRY\n",
      "LAST_UPDATE\n",
      "UPSITSNTDT\n",
      "UPSITENTDT\n",
      "SCRINTDT\n",
      "SCRINTST\n",
      "SCRTRDT\n",
      "SCRSITE\n",
      "SCRVISTYPE\n",
      "SCRSCHEDDT\n",
      "SCRVISST\n",
      "SCRRSNUM\n",
      "BLINTDT\n",
      "BLINTST\n",
      "BLTRDT\n",
      "BLSITE\n",
      "BLSCHEDDT\n",
      "BLVISST\n",
      "BLRSNUM\n",
      "COHORT\n",
      "TRACK\n",
      "CONSENTDT\n",
      "CONSENTST\n",
      "ENROLLST\n",
      "BIRTHSEX\n",
      "RAASIAN\n",
      "RABLACK\n",
      "RADEC\n",
      "RAHAWOPI\n",
      "RAINDALS\n",
      "RAUNK\n",
      "RAWHITE\n",
      "HISPLAT\n",
      "COUNTRY\n",
      "SCREENINGID\n",
      "SCREENDT\n",
      "SCREENTM\n",
      "CAMPAIGN\n",
      "REQAGE\n",
      "REQLOC\n",
      "DIAGPD\n",
      "PDMED\n",
      "DIAGAGE2\n",
      "RTRNLTR\n",
      "PARTNERID\n",
      "USQ1\n",
      "USQ2\n",
      "USQ3\n",
      "USQ4\n",
      "USQ5\n",
      "USQ6\n",
      "USQ7\n",
      "USQ8\n",
      "USQ8A\n",
      "USQ8B\n",
      "USQ8C\n",
      "USQ9\n",
      "USQ10\n",
      "REC_ID\n",
      "SCENT_01_CORRECT\n",
      "SCENT_01_RESPONSE\n",
      "SCENT_02_CORRECT\n",
      "SCENT_02_RESPONSE\n",
      "SCENT_03_CORRECT\n",
      "SCENT_03_RESPONSE\n",
      "SCENT_04_CORRECT\n",
      "SCENT_04_RESPONSE\n",
      "SCENT_05_CORRECT\n",
      "SCENT_05_RESPONSE\n",
      "SCENT_06_CORRECT\n",
      "SCENT_06_RESPONSE\n",
      "SCENT_07_CORRECT\n",
      "SCENT_07_RESPONSE\n",
      "SCENT_08_CORRECT\n",
      "SCENT_08_RESPONSE\n",
      "SCENT_09_CORRECT\n",
      "SCENT_09_RESPONSE\n",
      "SCENT_10_CORRECT\n",
      "SCENT_10_RESPONSE\n",
      "SCENT_11_CORRECT\n",
      "SCENT_11_RESPONSE\n",
      "SCENT_12_CORRECT\n",
      "SCENT_12_RESPONSE\n",
      "SCENT_13_CORRECT\n",
      "SCENT_13_RESPONSE\n",
      "SCENT_14_CORRECT\n",
      "SCENT_14_RESPONSE\n",
      "SCENT_15_CORRECT\n",
      "SCENT_15_RESPONSE\n",
      "SCENT_16_CORRECT\n",
      "SCENT_16_RESPONSE\n",
      "SCENT_17_CORRECT\n",
      "SCENT_17_RESPONSE\n",
      "SCENT_18_CORRECT\n",
      "SCENT_18_RESPONSE\n",
      "SCENT_19_CORRECT\n",
      "SCENT_19_RESPONSE\n",
      "SCENT_20_CORRECT\n",
      "SCENT_20_RESPONSE\n",
      "SCENT_21_CORRECT\n",
      "SCENT_21_RESPONSE\n",
      "SCENT_22_CORRECT\n",
      "SCENT_22_RESPONSE\n",
      "SCENT_23_CORRECT\n",
      "SCENT_23_RESPONSE\n",
      "SCENT_24_CORRECT\n",
      "SCENT_24_RESPONSE\n",
      "SCENT_25_CORRECT\n",
      "SCENT_25_RESPONSE\n",
      "SCENT_26_CORRECT\n",
      "SCENT_26_RESPONSE\n",
      "SCENT_27_CORRECT\n",
      "SCENT_27_RESPONSE\n",
      "SCENT_28_CORRECT\n",
      "SCENT_28_RESPONSE\n",
      "SCENT_29_CORRECT\n",
      "SCENT_29_RESPONSE\n",
      "SCENT_30_CORRECT\n",
      "SCENT_30_RESPONSE\n",
      "SCENT_31_CORRECT\n",
      "SCENT_31_RESPONSE\n",
      "SCENT_32_CORRECT\n",
      "SCENT_32_RESPONSE\n",
      "SCENT_33_CORRECT\n",
      "SCENT_33_RESPONSE\n",
      "SCENT_34_CORRECT\n",
      "SCENT_34_RESPONSE\n",
      "SCENT_35_CORRECT\n",
      "SCENT_35_RESPONSE\n",
      "SCENT_36_CORRECT\n",
      "SCENT_36_RESPONSE\n",
      "SCENT_37_CORRECT\n",
      "SCENT_37_RESPONSE\n",
      "SCENT_38_CORRECT\n",
      "SCENT_38_RESPONSE\n",
      "SCENT_39_CORRECT\n",
      "SCENT_39_RESPONSE\n",
      "SCENT_40_CORRECT\n",
      "SCENT_40_RESPONSE\n",
      "TOTAL_CORRECT\n",
      "upsitorder\n",
      "UPSITFORM\n",
      "UPSIT_PRCNTGE\n",
      "UPSIT_PRCTVER\n",
      "UPSIT_SOURCE\n",
      "UPSITLANGCNTR\n",
      "UPSIT_ORIG_ENTRY\n",
      "BIRTHDT\n",
      "YEAR\n",
      "AGE\n",
      "HYPOSMIA\n",
      "HYPOSMIA_PPMI\n",
      "target\n",
      "REC_ID_x\n",
      "PAG_NAME_x\n",
      "INFODT_x\n",
      "AFICBERB\n",
      "ASHKJEW\n",
      "BASQUE\n",
      "CHLDBEAR\n",
      "HOWLIVE\n",
      "GAYLES\n",
      "HETERO\n",
      "BISEXUAL\n",
      "PANSEXUAL\n",
      "ASEXUAL\n",
      "OTHSEXUALITY\n",
      "HANDED\n",
      "RANOS\n",
      "RAUNKNOWN\n",
      "ORIG_ENTRY_x\n",
      "LAST_UPDATE_x\n",
      "EDUCYRS\n",
      "COHORT_DEFINITION\n",
      "ENROLL_DATE\n",
      "ENROLL_STATUS\n",
      "STATUS_DATE\n",
      "ENROLL_AGE\n",
      "INEXPAGE\n",
      "AV133STDY\n",
      "TAUSTDY\n",
      "GAITSTDY\n",
      "PISTDY\n",
      "SV2ASTDY\n",
      "PPMI_ONLINE_ENROLL\n",
      "CONCOHORT\n",
      "CONCOHORT_DEFINITION\n",
      "CONLRRK2\n",
      "CONGBA\n",
      "CONSNCA\n",
      "CONPRKN\n",
      "CONPINK1\n",
      "CONHPSM\n",
      "CONRBD\n",
      "PHENOCNV\n",
      "DIAG1\n",
      "DIAG1VIS\n",
      "DIAG2\n",
      "DIAG2VIS\n",
      "DIAG3\n",
      "DIAG3VIS\n",
      "COMMENTS\n",
      "CONDATE\n",
      "ENRLPINK1\n",
      "ENRLPRKN\n",
      "ENRLSRDC\n",
      "ENRLHPSM\n",
      "ENRLRBD\n",
      "ENRLLRRK2\n",
      "ENRLSNCA\n",
      "ENRLGBA\n",
      "PD_Duration\n"
     ]
    }
   ],
   "source": [
    "#check if there is any duplicate in PATNO\n",
    "print(train['PATNO'].duplicated().sum())\n",
    "print(test['PATNO'].duplicated().sum())\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train_columns = train.columns\n",
    "test_columns = test.columns\n",
    "\n",
    "if train_columns.equals(test_columns):\n",
    "    print(\"Columns are the same\")\n",
    "\n",
    "for column in train_columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.362585616438356 27.402397260273972\n",
      "6.0 8.0\n",
      "40.0 40.0\n",
      "target\n",
      "0    34.467577\n",
      "1    22.215636\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "target\n",
      "0    32.726027\n",
      "1    22.078767\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "0.0    7.0\n",
      "1.0    6.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "0.0    10.0\n",
      "1.0     8.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "0.0    40.0\n",
      "1.0    30.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n",
      "HYPOSMIA\n",
      "0.0    40.0\n",
      "1.0    28.0\n",
      "Name: TOTAL_CORRECT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#sum all correct columns (that contain \"CORRECT\")\n",
    "\n",
    "#print mean and std of total_correct\n",
    "print(train['TOTAL_CORRECT'].mean(), test['TOTAL_CORRECT'].mean())\n",
    "#MIN AND MAX\n",
    "print(train['TOTAL_CORRECT'].min(), test['TOTAL_CORRECT'].min())\n",
    "print(train['TOTAL_CORRECT'].max(), test['TOTAL_CORRECT'].max())\n",
    "#print mean and std of TOTAL_CORRECT per target\n",
    "print(train.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "print(test.groupby('target')['TOTAL_CORRECT'].mean())\n",
    "#print mean and std of TOTAL_CORRECT\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].min())\n",
    "#now per HYPOSMIA\n",
    "print(train.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    "print(test.groupby('HYPOSMIA')['TOTAL_CORRECT'].max())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set created with 584 instances.\n",
      "Test set created with 146 instances.\n"
     ]
    }
   ],
   "source": [
    "# Save the train and test sets to new CSV files\n",
    "train.to_csv('data/processed/train_data_all.csv', index=False)\n",
    "test.to_csv('data/processed/test_data_all.csv', index=False)\n",
    "\n",
    "print(f\"Train set created with {len(train)} instances.\")\n",
    "print(f\"Test set created with {len(test)} instances.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
